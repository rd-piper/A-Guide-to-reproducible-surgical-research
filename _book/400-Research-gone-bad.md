# Research gone bad

[See Audiofile Link](https://surgery.rnsh.org/research-guide/_audio/4.oga)

<!-- - problems with research  -->
## Research integrity

*Integrity : the quality of being honest and having strong moral principles [Oxford English Dictionary]*

There are strong motivations for physician-scientists to publish research papers, including recognition and career advancement, gaining a qualification or consideration for a good post. It is obviously not good to view a piece of work as just another publication on the CV, since it should have its own justification. Nevertheless, inappropriate motivations to publish cannot be wished away, and it is important to recognise that it can lead to actions that can violate the integrity of the work.

In most contexts it is useful to think of research [integrity](https://en.wikipedia.org/wiki/Integrity) as broadly equivalent to ‘honesty’, but in practice research integrity goes well beyond that. Research integrity carries a requirement to ensure there is no deception, intentional or inadvertent, and also that there is no other unethical behaviour such as cruelty to animals or lack of care for patient personal privacy or agency. 

To ensure high integrity it is commonly agreed that it is not enough to rely too much on a person’s intent or goodwill (though that is important), but to have procedures and cultural norms that reliably detect and avoid undesirable behaviour. When we think of deception we think of dishonesty or fraud, but the easiest person to deceive is ourselves; the scientific method properly applied should minimise this so we do not credulously pass on a falsehood. Another example is that in human ethics there are in place processes and procedures that oblige us consider deeply the risks we may inadvertently expose a patient to. We need to use such methods and if we do not, we are not displaying proper integrity even if we do not mean to be bad.

Integrity also implies a steadfast attachment to a set of ethical principles. This carries with it in medical research a severely practical consequence, the obligation to apply to an ethics committee for permission to pursue a study where it concerns a human or animal subject. This partly converts an exercise in ethical behaviour into [] form filling, which is not fun, but as noted above it has the positive effect of pushing the applicant to scrutinise carefully important aspects of their work and to use standardised methods that are regarded as ethically sound. It is an awkward fact about research ethics that it involves reducing risks that are not at all obvious or central to research, such as risks around insecure storage of personal electronic medical data.

### The importance of not making things up {-}
Presenting or publishing data that has not been honestly generated or that has been made up for clearly fraudulent purposes constitutes serious research misconduct. Science, in theory, should not need policing since a finding that is wrong cannot be replicated so does not find its way into the accepted corpus. In practice there are usually severe consequences of fraudulent data, including the waste of time and effort to replicate it and the danger that clinical treatments based on fraudulent data pose to human health. Faked data usually involves the misuse of research funds that supported the researcher, so it is also a civil legal matter and may be a criminal matter. However, data fraud invokes such visceral dismay amongst scientists and physicians mainly because it betrays the implicit pact to trust each other to be always open, honest and receptive to criticism. Scientific institutions thus have strong ethical rules that render this severe breach of trust an employment-terminating matter at the very least. Such a rupture in this system of trust causes reputational damage to all involved, and betrays the trust of non-scientist taxpayers and charity-funders in scientific institutions.

### The importance of not making things up even a little bit {-}
While fudging or copying a small inconsequential figure or numerical data may not seem important, it really is. No medical or scientific institution can tolerate even the suspicion of dishonesty, so it is a central part of the culture of science that we learn to police ourselves in minor things, and we cannot function as scientists if we allow ourselves to lapse in this regard. This is not because these minor things are not minor (they may well be), rather it is because it is a failure of self-policing, as well as a failure to adhere to an ethic that so crucial to the research endeavour. In clinical research this can even include accepting incomplete datasets as if they were complete, or knowingly analysing datasets that may not have been rigorously or prospectively collected as described

### What if it is someone else? {-}
A really thorny problem arises if we become aware that someone else may be lacking in research integrity. This needs to be tackled with sensitivity as it would be in any other walk of life, as suspicions can be wrong. It may be best to discuss misgivings with the person involved to ensure any problem is fixed and not repeated. Preventing someone from acting improperly in a minor matter is doing them a big favour if it prevents the consequences of a worse infraction (e.g., publishing fraudulent data) later on. It is certainly advisable, and it may be mandated, to consult with a mentor or an official with responsibility for research integrity. The path taken is very context dependent and depends on the guidelines of the institution (they all have them) so it is best to consult these. 

### Human research ethics committees {-}
No committee can oversee clinical activities in detail, so various approaches have been developed to deal with these issues. From a researcher perspective, it broadly means training and ethics application forms. Most issues around research ethics arise from a lack of awareness of the researcher as to what is expected and is permissible. Training, form completion and ethics committee engagement can address these issues effectively, but they also enforce openness about researcher intentions through the requirement to document them beforehand. A perennial issue is that the number of ethical[] concerns can only rise over time as new problems and procedures appear, so the amount of work involved in ethics committee applications also rises over time, despite occasional efforts to streamline processes. 

### Study design {-}
Less often considered under the heading of research integrity is the requirement for good study design. A poorly designed, underpowered and badly analysed study runs a severe risk of generating misleading data, due to biases and random chance. This must be avoided. There is an aspect of data analysis formally (and rather curiously) named “Vibration of Effects” which is the ability of a dataset to be analysed by many different statistical tests to give a range of different outcomes, meaning they lack robustness. This needs serious thought.

### Conflicts of interest {-}
This is an important issue defined as researchers having some financial or other strong interest in the outcome of the work. This can at the very least lead to biases in data interpretation. Where these conflicts cannot be avoided the main recourses are to minimise them where possible, and to declare them openly and let others decide if this irredeemably biases the study conclusions. 

## Narratives : necessary but dangerous.

An often heard piece of good advice to a new researcher attempting scientific prose is to “tell a Story”. This Story is sometimes referred to as a Take-Home Message, the thing that (it is implied) the reader or audience should remember long after the bothersome details have faded from memory. We can think about the Story in two different ways. In the first way, a Story highlights a cogent point or set of points, forming a focus of the work that points the reader towards a clear and easily understood conclusion. This makes the task of writing much simpler.  As well as a focus and a structure, the Story can give the writing a certain vigour and coherence and, if done well, makes the writing compelling and persuasive.  It is worth noting that undergraduate students in particular often write up their research projects as a catalogue of apparently unlinked data and arguments that inspire little engagement or insight and with no obvious rationale. This is a consequence of lacking a well thought out Story. The data and analyses may be central to reporting a study, but the Story will link these disparate elements to give them a form needed for proper communication and understanding. Note that it is common to summarise or declare the Story in the title of the document, which is why it can be helpful to work out a draft title to help the writing process.

The second way to think of the Story is as a form of information compression. The human mind takes in information very efficiently in narrative form and as it allows us mentally to fill in implied (but not supplied) information. Consider two versions of a statement* :
  *“The queen died and then the king died”*, and 
  *“The queen died and then the king died of grief”*. 

Only two words different, but while the first is a simple statement of a sequence of events without even a sure link between the affected individuals (apart from social rank), the second version is richer and contains hints of a plot with personal motivation, causation and personal relationships. All from two words. This is the power that narrative has, and why it is so important to scientific writing. It compresses information by drawing on knowledge the reader already possesses (we know many implications of ‘grief’ and ‘dying of grief’) as well as a store of stories we inherit from our culture(s) with its many stories of royalty, grieving and dying that we have imbibed. Narratives are a natural and powerful way to learn, and seem to be a central and universal feature of human psychology. Their importance is evident in literature, poetry, politics, advertising and many other fields. But there is a dark side that lurks in a narrative: the Narrative Fallacy.

Nature, by and large, does not have narratives; they exist only in our head. A good narrative can impel people to think that a conclusion is logical and perhaps inevitable: if A and B followed by C then D, then we all know that E happens next. The conclusion of E may not be wrong, but it may not be as certain as it seems. This is a type of cognitive bias or fallacy – that is, it is true if the Story sounds good. Such biases can fool us badly but, happily, in the scientific method we have thinking tools that make us question and check and with luck overcome the effects of this and other biases.

To summarise, narratives are a very useful, a superb communication method, but be careful not to take it too seriously.

* based on an oft-quoted example from E.M. Forster Aspects of the Novel, 1927.

## Psychological biases in scientific research 

People do not process data as computers do; that is, as a sequence of logical processes that follow instructions to give infallibly correct answers. That much is evident to anyone using a computer, but the differences are very profound. With careful effort, humans are capable of strictly logical thought, but everyone knows, that everyone else is not very logical most of the time.  

It is easy to demonstrate that the human mind uses shortcuts or approximations without much awareness of doing it. Indeed, driving a car (for example) would be impossible if we had to consciously think of every action. We catch a ball by rapid reflex, quickly decide where to lunch in a city full of cafes, cross a street unhurt and recognise a friend in a crowd of strangers. These and a million other amazing things we do with those rapid approximations (or heuristics) that work most of the time, good enough for hunting wildebeest in the savannah and not bad in a modern city. They also enable strategies to make decisions under uncertainty, such as choosing birthday presents for teenage nieces, where logic is not enough. 

These unconscious shortcuts and approximations lead to a curious shadowy side of the human mind:  the cognitive biases. These are the systematic thinking errors we make. They are of crucial importance in virtually every field of human endeavour, but especially in scientific research. The scientific method is, in large part, a way to overcome errors these biases cause. There are now a very large number of cognitive biases that are well documented and defined; below are a biased sample of the more interesting ones. 

### Confirmation bias {-}
All people have a strong tendency to overestimate the importance of information that support their existing beliefs and underestimate information that does not. 

### Hindsight bias or outcome bias {-}
Things that happened seem virtually inevitable in retrospect, while things that did not happen were bound not to happen.

### The availability heuristic {-}
A very weird but common bias, is to overestimate the importance of information immediately available to us. An important example is that we estimate the prevalence of crime by how easily we recall instances of it. For example, people consistently overstate the toll of terrorist victims (which are fresh in the mind with incidents constantly reported) yet understate the toll of traffic accident victims.

### Survivorship bias {-}
Related to hindsight bias, a common mistake that comes from considering surviving examples. Thus, entrepreneurship seems easy as we hear little about businesses that fail and disappear. When we seek to emulate those who were successful, without recognising those who did similarly were unsuccessful, we fall prey to this bias. Bill Gates dropped out of college, but not many college dropouts do as well.

### Overconfidence bias {-}
We obviously and strikingly overestimate our abilities while underestimating our overconfidence, leading us to take far greater risks than otherwise, hence the recklessness of youth. 

### The Dunning-Kruger effect {-}
Those with high expertise have a strong tendency to underestimate their expertise and those with low expertise have a strong tendency to overestimate their expertise, which interacts badly with the overconfidence bias.

### Narrative bias {-}
The tendency to put strong faith in information that fits well with a good story.

### The ostrich effect {-}
A tendency to ignore dangerous or negative information, which is related to confirmation bias.

### Selective perception {-}
The tendency for expectations to influence how strongly we perceive things. This is particularly important in pain management.

### Zero risk bias {-}
Humans are very poor at estimating risk, so even small risks can cause stress, leading us to prefer certainty even though it may be counterproductive. This is an important consideration in counselling patients about procedure risks and disease outcomes. 

### Pro-innovation bias {-}
The tendency to overestimate the usefulness of something if it shows novelty.

### Anchoring bias {-}
A puzzling tendency to over-rely on the first piece of information encountered. Thus, the first number suggested tends to frame a negotiation over price, or affect a subsequent quantitative estimate.

### Clustering illusion {-}
The tendency to see patterns in random events or data, such as seeing faces in the clouds.

### Choice support bias {-}
The strong tendency to feel positive about something you have chosen or publicly support. This is why your own dog is great even if no one else thinks so.

### Recency bias {-}
Believing (for no reason) that newer data is more reliable than older data, and that recent trends have a high likelihood of continuing into the future.

### Bandwagon effect {-}
That well known tendency of other people to adopt a belief based on how many people believe it.

### Blind spot bias, or cognitive bias bias {-}
Lack of recognition of our own biases while being aware of the biases in others.

### Research and the scientific method {-}
Even though we are influenced and affected by many different ways to make mistakes, there are also  many different ways to overcome them. In science, the most obvious way is the systematic checking of ideas and concepts against reality. There are many ‘thinking tools’, such as the refinements of philosophical logic (e.g., induction, falsifiability and paradigms), statistical analysis, controlled experiments, peer review, seminar presentations, writing grant applications and a plethora of other features of the culture of science. Together, these are potent (if not always efficient) ways of tackling biases. Even so the struggle continues, as is evident in the low rates of reproducibility emerging in many areas of research. First and foremost, be mindful of biases as they never go away. 

## The replication crisis in medical research
Many articles have appeared over the last ten year years in serious and august journals such as Nature and Science describing (and expressing alarm at) the low reproducibility of scientific findings in many fields. This is a very significant issue, and bad news because it is reproducible observations that form the bedrock of science. For this reason it is important to know something of the nature of the problem and the think about the implications for one’s own work. 
When an observation or experimental result is easily replicated by others it can be accepted as solid progress, can be used as a verified fact in scientific discussion to support arguments and is a sound basis for further progress. However, replication studies attempting to reproduce the central findings of prominent published work have indicated that an alarmingly large proportion contain flaws [@Ioannidis2014-tc; @Begley2012-uy; @Allison2016-xz]. This may not be entirely surprising since most worthwhile scientific studies are technically hard to perform, which makes replication difficult, but the problems go deeper than that [@Begley2012-uy; @Allison2016-xz]. Poor replication may go some way to explain why so many promising drug targets fail in the early stages of drug development. Clearly, however, there must be implications for a poor clinician trying to avoid wasting time on a flawed project.

### What studies can be trusted to form a basis of new work? {-}
There is no straight answer to this, only general rules of thumb. A simple one is that older studies have more information that help judge trustworthiness. Thus, if a study has been cited by others and its outcome repeated successfully over a long period then more weight can be placed on it. A study on a clinically important topic from 10 years ago has had plenty of time to be reproduced and built upon, and if it has not (perhaps evidenced by absence of later related publications) then it may not be wrong, but it needs to be marked down. Unlike an old study, a recently performed study may be perfectly executed but it has not had time to earn as much trust.

Studies where the outcomes or observations are made by a simple method, giving less chance of systematic errors, can be more robust. Another good indicator is that a study reports secondary independent parameters that help to validate the main outcome. Thus, patients receiving treatment may improve a targeted function (e.g., limb mobility, cardiac output, urine flow) and show sustained lifestyle improvements that should be an expected correlate. Other indicators include blinding of researchers during the study, publication of all data (not just selected data), and appropriate statistical tests.

### What about peer review? Are reviewers no good? {-}
Peer review must be viewed as a filtering system of quality control and is well accepted as far from perfect, although it seems to be the least imperfect (and most practical) devised so far. Alternative assessment systems exist such as online feedback on bioRxiv preprint manuscripts but have yet to achieve traction. There are two particular issues of note with peer reviews. The first is that reviewers can only judge what is put in front of them and ferret out flaws they can detect, and most do a good job at this. A perfectly executed study may give outcomes that no one can repeat because of some obscure issue: an unstable drug, faulty computer code or a biased patient sampling not evident in the manuscript. 

The second issue is that journals (and peer reviewers) can themselves be part of the problem. They expect novel exciting and important results. This creates huge perverse incentives for authors to provide that. If a fascinating outcome has not been seen by 99 researchers but is seen by 1, whose study will be published? This is the well recognised problem of publication bias, and is proving hard to fix.

### Why is this happening? {-}
There is a long list of factors adding to replication issues. Pressure to publish (for career enhancement and obtaining funding to support the work), poor usage of statistical techniques (very common), protocols that are difficult to follow and studies that have a high risk of systematic bias in collecting data - these are all examples of contributing problems. Making all this worse are the systematic and cognitive biases that affect researcher judgement - we fool no one better than we fool ourselves.

### What can be done to avoid this type of trouble? {-}
There is no panacea, but the classic scientific virtues will go a long way: good experimental design, robust critiques of the work from many sources, a high quality of statistical analysis and interpretation, good mentorship, extensive documentation and a painstaking approach to performing well even the smaller aspects of study protocols. It is also important to be aware of the impact of our own cognitive biases, which we underestimate, and the fact that the scientific method has evolved to overcome these biases. This can be seen in clinical studies, for example, in the need to take seemingly excessive trouble over blinding of both subjects and researchers. 

### Is there no hope? {-}
There is hope, as replication issues have gained a great deal of recognition, leading gradually to improvements. Publication bias is being addressed, and publication of purely replicative papers more acceptable. Preregistration of clinical trials and systematic reviews are now a requirement. A new focus on statistical rigor is evident in journal submissions, and more powerful analysis methods gaining traction. One is the use of a Bayesian statistical framework, which this needs some trouble to use. This views a piece evidence not so much as proof but as a modifier of a prior belief, to a degree that depends on the strength of the evidence. This approach leads to a conclusion we can instinctively accept: that extraordinary claims need extraordinary evidence, but less so with less extraordinary claims. Thus, smudgy photos may be proof to a court of a speeding offence, but poor proof of Martians among us.  

Another hope is the emerging clinical practice of systematic review and meta-analysis of prior studies. Not only does this involve careful assessment of biases but it aggregates studies that have their own features (good and bad), and so a diversity of practices may average out and ensure that outcomes can have some weight placed on them.

There are also many other emerging recommendations designed to help ensure better reproducibility, some of which can be surprising [@Raphael2020-pw]. It is a good idea to read up on the subject and take it seriously. A bracing place to start (if a little technical) may be the famous inflammatory essay by John Ioannides “Why most published research findings are false” in PLOS Medicine 2005 [@Ioannidis2014-tc] or the interested researcher can enter the terms “scientific reproducibility” or “scientific replication crisis” into a search engine and stand well back. 


* Begley C G et al. (2012). "Drug Development: Raise Standards for Preclinical Cancer Research". Nature. 483: 531–533. PMID 22460880.
* Allison DB et al. (2016) “Reproducibility: A tragedy of errors.” Nature. 530:27-9. PMID: 26842041
* Ioannidis, JPA (2014). "How to make more published research true " PLoS Med. 11:e1001747. PMID: 25334033 

## Survivorship bias
This a logic flaw that is very pernicious because it can be so hard to spot. Survivorship bias is commonly seen in popular journalism, but mercifully occurs far less in scientific work since the scientific method usually shields us from it. It is important, however, to be aware of it any clinical research setting, particularly when designing new studies or constructing research questions and hypotheses to be tested.

To illustrate the effects of this bias, imagine that a few months before September 11 2001, US aviation authorities bring in a rule requiring the aeroplane flight decks be inaccessible during a flight, rendering impossible the terror attack on the World Trade Centre and Pentagon*. Imagine also that an international pandemic prevention team rapidly identified an infectious new coronavirus in bats in central China and within a week produced a diagnostic test and enforced a short lockdown in towns nearby, blocking its spread. In both of these scenarios history would be changed, but later we might complain about inconvenient restriction of movement for aircraft captains or question the huge cost of a pandemic preparedness, since ‘nothing’ had happened. These illustrate some of the traps and consequences of Survivorship bias – we cannot see things that do not happen, but which might have.

### What exactly is this bias? {-}
Defining this bias in simple terms is not easy. We can formally describe it as failing to see things that do not survive a selection process; we thus fall for the bias when we see only those things surviving selection and assume they are representative of the whole. However, this type of definition of Survivorship bias is not very helpful as it does not convey why the bias is so often invisible, which is the real problem. Often (as in the scenarios above) it is because we are only dimly aware that a selection process even exists. In other cases it is complicated by other mental biases such as Hindsight bias (i.e., considering that things that happened in the past necessarily had to have happened); this bias can, indeed, be seen in the scenarios above. It is usually more helpful to work through illustrating examples.  

### So what examples might help? {-}
Examples from popular journalism are everywhere, such as discussing the great strategies of successful people in a field (such as wealth accumulation or finance) without considering those people who followed the exactly the same strategies but who failed. The latter group are hard to see since they tend to exit the arena. Clearly, adopting a strategy based on such incomplete data is dangerous as it leads to over-optimism.  

For another example, we admire the beautiful and functional buildings built in the past without considering that any ugly and badly constructed building were demolished years ago. Most ancient Roman dwellings, for all we know, may have been horrible. Historians by the nature of their work are highly subject to Survivorship bias as they study documents and artefacts that have survived the years; they cannot easily study what might have been or what was lost. 

A slightly happier story can be seen in the introduction steel helmets in world war one trenches. When French Adrian and British Mark 1 helmets were worn there was an increase in soldiers being treated for serious head wounds, and it was widely supposed that there was to some defect in helmet design. In fact careful analysis of the data by army medical staff showed that this was actually due to improved survival of soldiers hit on the head. i.e., without helmets soldiers would not survive to be treated for their head wounds. This illustrates that the logic error arises from the perspective of the data-gatherer who only notices the increases in head wounds, but it also shows how this bias can be overcome.

Lastly, there is the apocryphal but entertaining story of the scammers who sent weekly messages predicting stock market up or down movements to potential investors. To half they sent predictions of up, the other half predictions of down. If the market went down they sent no more messages to those who received a prediction of up; to the remainder, half were sent new (random) messages predicting up and half predicting down. And so process repeated. This continued until most investors were dropped. By this time a few investors had had a long run of correct predictions and with such ‘proof’ of the scammers competence, sent them money. The trap was the newly impoverished investor’s perspective: they could not see they were a survivor of a random selection process. 

### Where does this bias crop up in clinical research? {-}
The most obvious is in publication bias, where only exciting positive trial results are published and we cannot see the unpublished negative results. This can be dealt with using the techniques of systematic review and meta-analysis. When designing a study we must be alert to how survivorship bias may affect outcomes, for example if a subset of a group under study may systematically drop out over time.  Hypothesis construction may be based on datasets where outcome evidence may be missing or biased; it is not necessarily fatal to have a flawed basis for the project hypothesis (since it will be tested), but it will affect weighting of priorities and funding justifications, and may lead to time wasted.  

Survivorship issues can crop up whenever a selection or stratification step is made in data analyses. Interpretation of clinical outcomes becomes a problem if some outcomes are not recorded (so cannot be seen), and this is classified as a study design flaw. This is well understood in clinical trials and is dealt with by the technique of censoring.

### How can a researcher minimise or avoid this bias? {-}
Half the battle is simply to recognise that it can occur, and it is useful to think about where it might happen in your work – which is really the point of an article like this. It can be an example of an unknown unknown, that is, a problem that we are not aware could exist so we are completely blind to it. That being the case the way to tackle this is by adopting a routine strategy to detect it, such as checking if all participants entering a trial have an outcome that is recorded. 

Another approach using data is to construct a prediction model which can then be cross-validated on other datasets. This can reveal the presence of biases, including survivorship. 

* This example taken from Nassim Nicholas Taleb book “The Black Swan”.

## Parkinson’s Laws and related project planning pitfalls for researchers

While project management need not be a formalised process it is important to consider it carefully. As if research were not hard enough, there are a number human peculiarities that creatively combine to frustrate project advancement, so a few are briefly noted here as an alert to some general obstacles to watch out for. Some of these are helpfully named after those that first identified them, as with the first example named for C. Northcote Parkinson and his eponymous book.

### Parkinson’s Law {-}
The great law of bureaucracy, namely that "work expands so as to fill the time available for its completion". Not disastrous in itself, but gives a warning about an odd trap often seen in project work. A time allocation may be appropriate for a project, but often a deadline it is some distance in the future so can be admired from afar. That means that for now endless discussions and meetings can be scheduled about it, and collaborators may not necessarily prioritise it. Only when the deadline nears do substantive things tend to be done, but the potential problems with this approach is self-evident.

### Fredkins paradox and Buridan’s Ass (or Buridans principle) {-}

*“The more equally attractive two alternatives seem, the harder it can be to choose between them—no matter that the choice itself may matter little.” -- [Edward Fredkin](https://en.wikipedia.org/wiki/Edward_Fredkin)*

A similar idea to this is the story of Buridan’s ass, a poor animal that starved to death because it was sitting between two equally alluring piles of food. 
It can often be more important to make a decision than be paralysed making the perfect decision.

<div class="figure" style="text-align: center">
<img src="https://upload.wikimedia.org/wikipedia/commons/2/22/Deliberations_of_Congress.jpg" alt="Buridan's Ass" width="100%" />
<p class="caption">(\#fig:buridan)Buridan's Ass</p>
</div>

### Bikeshedding {-}
Also called Parkinson’s Law of Triviality from an example given by Parkinson himself. This concerned a council committee asked to approve the local construction of a nuclear power station and a shed for parking bicycles. The committee would take far longer to approve the bike shed that the power station, since no one really understands huge undertakings like power stations but everyone on the council has opinions on bike sheds. While a nuclear power station would be more controversial now than in the 1950s, the point is that easy trivial things can take up more time than big hard things, and we need to watch out for this.

### Hofstadter's Law {-}
"Hofstadter's Law: Complex tasks always takes longer than you expect, even when taking into account Hofstadter's Law." Everyone has encountered this.

### Procrastination {-}
Many books have been written about procrastination and how to overcome it, and everyone has their own weaknesses in this regard. If procrastination (i.e., avoiding work on a project) is a persistent problem, the first task is to recognise that it is so, and think why. Actually finding ways to overcome it is another matter but a great deal of helpful advice is available and here the internet is your friend. This is a huge subject that cannot be summarised in any detail here, but typical approaches involve dividing a task into many small tasks that are less easily avoided, good time management, examining the root cause of the avoidance and (in extremis) giving the task to someone else.   

The tyranny of small decisions, and of path-dependence
This refers to the fact that lots of small decisions by many people taken for perfectly good reasons at the time can aggregate over time into something undesirable. In the same way many reasonable small actions affecting a project can aggregate to something bad, and the ways forward for a project limited by decisions made in the past. Such path-dependent problems can be hard to solve without starting from scratch which is often a costly option.

### Unexpected developments and mission creep {-}
Any scientific project can take a left turn, where some curious or exciting experimental result occurs that distracts from the core business of the study. This can lead to changes in the research plan and makes the mission or scope of the project larger. This needs cautious management as it can result in missed deadlines if not brought under control. A new and unexpected observation may be an opportunity but may be a distraction and it takes some wisdom and good mentoring to tell the difference.

### Unclear responsibility or accountability {-}
Who is responsible for making sure that an aspect of the project is done? What will keep that person honest or accountable? If there is nothing, how can that be handled by the person in charge of the project?

### Soft deadlines {-}
Soft deadlines, i.e., deadlines that pass with little consequence, can be a real pest because no one respects them. If a project deadline is a definitive (i.e., hard) one, then deadlines agreed for the various milestones of the project have to be adhered to and usually are. In contrast, soft deadlines can be a paradoxically hard to keep on track with, and may be a big contributing factor to Hofstadter’s Law. Combined with Parkinson’s Law, soft deadlines can be serious nuisance.

### Communication {-}
It is important that this is timely and done well to make sure that everyone working on a project is kept informed but equally important the that they are not overburdened with irrelevant material. There are now many online tools to facilitate this.

 
