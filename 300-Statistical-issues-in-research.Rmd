# Statistical issues in research

[See Audiofile Link](https://surgery.rnsh.org/research-guide/_audio/3.oga)

## Statistical analysis of data – a random sample of thoughts  
Statistical analysis is an enormous subject which cannot be seriously addressed in a short article, but below are some brief points to consider when designing, executing and analysing a study. 

```{r cars-plot, fig.cap="The Caxton Celebration", echo=FALSE}
knitr::include_graphics(rep('images/The_Caxton_Celebration.jpg', 1))
```

### Statistical thinking needs to be done before the study begins {-}
Researchers often think of data as needing statistical analysis after it has been collected, but statistical modelling should be used from the earliest stages in the design of the study. That is, the study should be designed at the outset to give the best chance of generating informative data. Usually in medical projects the most important consideration is statistical power, summarised below.

### What is your level of statistical training? {-}
Any researcher that has a good grasp of normal distributions, probability, p-values, t-tests, ANOVA, chi-square tests and non-parametric testing has a good enough understanding to run simple analyses and should be at a level to discuss analyses and planning with a data manager or other statistically trained person. A researcher without those skills then needs to tread very carefully, and will need an expert mentor to consider and explain this aspect of the project, and provide some basic training. A researcher with relatively high skills so that, say Cox proportional hazards model, Kolmogorov-Smirnov tests, Bayes’ theorem and Kaplan-Meier curves hold no fears, then that researcher is better equipped. Even so it is always worthwhile to take time to discuss with peers and mentors about the methods and outcomes of the project statistical analysis.

### Statistical significance does not always mean medical significance {-}
If patient weight is monitored as part of a study of a weight reduction therapy and that therapy causes a statistically significant decrease of 10g, no one would care. Large datasets in particular can sometimes give highly significant results when a statistical test is applied but the observations may not be meaningful if they only show up tiny changes that implying the intervention has negligible, if measurable, effects []

### Confidence in significant differences {-}
When applying a t-test to compare the effects of a surgical intervention and sham intervention on a patient interleukin-101 (IL-101) levels, there are three types of outcomes possible. Firstly, treatment may result in a significant increase in IL-101, i.e., the mean level of the treated group is significantly higher than that of the sham group. Secondly, there could be a significant decrease. 

The third possibility is that no significant difference in IL-101 seen between surgical and sham groups. A little more formally, we would say that the mean IL-101 levels seen in the treatment group lies inside a grey zone, the confidence interval, that lies around the mean of the sham operated group. If our treatment group data mean ends up in that zone then the difference was not statistically significant. That means either the intervention had no effect (so the apparent difference seen is only due to random variation in the data) or the effects of the surgery was just too small to see because it is swamped by natural variation (a.k.a. noise) in the data. The latter means the power or ability of the study to see an effect may be insufficient, just as a magnifying glass cannot help you see a bacterium. We cannot distinguish lack of effect from lack of study power; so if we can’t see bacteria, either they are not there or the magnifying glass is no good. We cannot be sure until we buy a microscope. 

### So a statistically significant result proves an effect exists? {-}
Evidence for an effect yes, absolute proof no. When the surgery does result in a significantly changed IL-101 level it means there is a low probability of that the data (or data more extreme) could be observed due to random chance. How low the probability is depends on the study, but is often 0.05 or 0.01. Statistical inference can be a slippery thing; always involving layers of interpretation and depends critically on the right statistical model being used.

### With great power comes great reproducibility {-}
From the above, it follows we can only show a significant difference if the surgery effect is large enough that it is not overwhelmed by the natural random variation in the data.  One way to reduce such random data variation is to increase the number of patients measured. If we do that then the surgery effect measurements may escape the grey zone, so that an effect (if it one really exists) can be seen. This increase in number of measurements causes an increasing in the ‘power’ of the experiment. In effect this is the power to spot a small but true difference between groups, though that is not its formal definition. Power can be estimated, and used to determine the number of patient measurements we need to make. It is crucial to know this for the study design, so that power is high enough to give a good chance of seeing a real and reproducible difference. Otherwise we are wasting our time and our patients’ time on the study, which has ethical and cost implications

### The hidden horrors of multiple testing {-}
When comparing mean values of two groups (say, placebo and a drug treatment) we may perform a significance test, such as a t-test. This give us a p-value representing the probability that difference between the groups (or a bigger difference) is due to random chance. We conventionally accept a p-value below 0.05 as significant though this is not very stringent, and 0.01 is better.

However, if we make two comparisons at the same time (e.g., comparing placebo with drug A and with drug B) and if both comparisons give a p-value a smidge below 0.05 then the chance that one of the two results is due to random chance is almost 10% (p=0.0975), which is no good at all. This is because multiple comparisons mean that p-values must be combined and adjusted so that we are not fooled by a false positive result. One very common method for this is the Bonferroni adjustment. 

In sum, multiple comparisons give a very different outcome to single comparison. It is like flipping a coin – chance of getting a head is 50% with one flip once but flipping 20 times makes it certain.

### Torturing the data till it confesses what you want {-}
With large datasets it is possible to perform significance testing on many parameters, and if nothing looks good you may look for significance in a subset of the data. For example, if the test of a drug finds no effect then you might notice that if you exclude patients over 80 year old it looks more promising; this may reduce the p-value to below 0.05. This procedure is bad, it is called data torture and it is horrible, but surprisingly popular. A hypothesis test must be designed before the data is obtained, not after the researcher had a chance to squint at it. That said, such post hoc subset analyses may suggest a useful idea for a new hypothesis for testing in later studies, which is fine.  A good source of information about these and related statistical sins is found in the short classic How to Lie with Statistics by Darrell Huff (Norton & Co, 1954) if you can locate a copy.

### But is it normal? {-}
Most statistical tests (e.g., t-tests) assume that the data follows a normal distribution, so when it is plotted out the data has the distribution of a bell curve. This can be tested mathematically, which is less trouble than plots and more objective (after all, how bell-like is your curve?) but it if turns out your data does not follow a normal distribution then t-tests will tell you lies. There are ways around this problem, the most straightforward of which is to use a non-parametric statistical test that does not assume normal distribution of data. Thus, instead of a t-test use a Mann-Whitney test. Bear in mind, however, the power of these tests is less, so they may return a false negative result.

### Statistics: not the only way. {-}
Statistical analysis is not the only way to interrogate your data. There is an emerging field of machine learning which can spot patterns in the data when statistical analysis cannot. This can be worth considering, but it is not for the faint hearted and is generally only useful for very large datasets. Indeed, it often gives outcomes that are hypothesis-generating rather than analytical in nature.

## Power calculations and why they are needed
When designing a clinical study we must decide how many study subjects will be included. This is required for planning, budgeting and ethical approval. Determining the minimum number of individuals to enrol in a study requires a statistical power calculation, which is not difficult in practice but needs an understanding of some statistical theory. Here, we discuss the ideas that underpin power calculations.

### A hypothetical clinical trial {-}
Consider first an imaginary study we have completed called the DOSH trial which compared a treatment group to a control (placebo) group. The DOSH trial investigated whether a drug treatment makes people richer, measured by bank balance magnitude, BB. The trial estimated whether an effect of the drug on wealth, BB, exists and the size of the treatment effect is, i.e., how much BB was increased after treatment. Note that the larger the treatment effect is then the easier it is to see (and to demonstrate) that it exists using statistical testing. 

### The importance of being significant {-}
Demonstrating that a drug effect on BB ‘exists’ means that we can show that the differences seen between the treatment and control group wealth are unlikely to be due to random variation or chance alone. This is decided is based on statistical hypothesis testing, and for this the DOSH trial used a t-test analysis of the mean BB levels of in the two groups, which takes into account the natural random variability seen in wealth levels. Crunching DOSH trial numbers found that the chance of the observed increase in BB in the drug treated group occurring purely by chance was 4.5%. This was regarded as statistically significant as it was less than the selected 5% significance threshold. Thus, we consider that there is a difference between the two groups that is unlikely to be due to random chance variations. We therefore infer that the drug really did increase bank balance levels, and we can estimate on average how much more money people had with the treatment. Job done.

### Power and what it means {-}
All of the above relates to a study that is already finished, so now suppose that we need to do another imaginary study called MoOLA on different population. How many individuals do we need to recruit to MoOLA so that we will have a 80% chance of seeing a similarly significant result to DOSH? This question relates to the power of the new study. If there are too few people recruited and it will be difficult to see a statistically significant difference of 5%. In this case the study is underpowered so you are wasting everyone’s time as an effect will not be detected even if it is there. The ethics committee will not take kindly to it either. If there is a massive abundance of subjects investigated the study may be overpowered, which is far less of a worry (and rarely happens), but is also wasting resources and the ethics committee will again not be pleased.

### How to estimate study power {-}
To work out the number of patients needed to power the MoOLA study to see a statistically significant outcome we need some prior information about the key parameter. Since we have the data from the earlier DOSH study we can use that. We first decide:

+ How big is the effect size we want to detect in MoOLA
+ Consult the data from DOSH to find how much BB varies by 
+ We choose a significance threshold (here 5%) 
+ We choose a tolerable false positive rate, usually 80%. 

This is enough to calculate n, the number of subjects needed. To do this with the information above if we define Diff as the previously observed mean difference between treated and control, SD as the observed standard deviation and SES the standardised effect size then:

$SES = Diff/SD$          and        $n =     16/(SES)^2$ 
                  
### The need for prior data {-}
Notice that for MoOLA we had prior data (DOSH) to help us with the power estimate but it is possible that no similar study of a lucregenic drug has been done before. There again, the prior data only has to provide an estimate, so may be from a different but related treatment; some thought needs to go into what is appropriate. Small (i.e., underpowered) pilot studies might be alright for this. Also note that the power or n estimate only relates to measuring parameter M; if other parameters are of equal interest power and n may need to be adjusted accordingly.

### Significance thresholds and the pain they cause {-}
It is important to note that the 5% significance threshold used above is both arbitrary and low. The assumption that random chance effects do not exist if their probability is below 1 in 20 is commonly made, but not very robust. The sad truth is that many studies give conclusions that are weak because of this, but power (and hence significance) is dependent upon the number of individuals studied (n) and in reality there may just not be enough study subjects to go around. Note that we have to be alert to the possibility that while an effect on BB may not explained by random chance, it may not be due to the treatment either but rather some sort of bias in the study design. 

### Other types of studies {-}
Note that the above description relates to a small clinical trial, but the same or similar statistical approach may be used to estimate power in other types of studies, such as observational and time series data or animal studies, though the power calculation method may differ accordingly.


## Systematic Reviews – A Short General Introduction
Clinicians need to know whether a treatment may be effective for their patient or, indeed, if it works at all. Published clinical studies are the main source of authoritative and objective information but there are problems in simply accepting these at face value: studies vary enormously in size and quality and may contain biases (i.e., systematic errors) that undermine their conclusions. To make matters worse, there is the problem of publication bias, whereby not all studies are published. Thus, for example, exciting first data on an innovative therapy may be published with fanfare, while later boring studies showing that the therapy is useless struggle to be published at all. As a result, unsuspecting clinicians performing literature searches see only the positive results and so can be badly misled. These problems can, at least in principle, be dealt with by systematic reviews.

### Unsystematic reviews {-}
A ‘literature review’ conventionally refers to an article that paints a detailed picture of the general current state of knowledge and thinking in a subject or field. Such a review is always supported by an accompanying extensive list of cited papers, bibliographies that are themselves useful resources. Such reviewing has a long and honourable history, but in recent years as emerged the more specialised and focussed type of article, the systematic review; the older type of review is usually referred to as a “narrative” review to contrast it. 

### What are they for? {-}
A systematic review seeks provide a direct answer to a focussed clinical question, and assesses the strength of evidence that supports that answer. The question addressed may be simple but it is narrow, such as whether a specific treatment is effective for a condition in a particular category of patient. Secondary questions, such as optimal dose or the frequency of complications may also be addressed. Systematic reviews promote evidence-based medicine as championed by the Cochrane Organisation, a resource supported by medical institutions worldwide, and are regarded as a very high level of clinical evidence.  

### What is systematic about it? {-}
Scientific endeavours are all systematic in some sense, but here it means the whole review process itself is systematic, not just its methods and analyses. It follows a set of prescribed and standardised procedures. For example, a systematic review performs meticulous and exhaustive searches for all accessible clinical studies that address the question in hand, and details of that search process are declared. This includes studies found outside of standard journals or in languages other than English; this aims to overcome publication bias. 

### How is a systematic review performed? {-}
That subject takes up entire books, with many aspects still quarrelled over. However, the general conduct of a gold standard systematic review is widely agreed. 

A systematic review is performed using a transparent, orderly process to identify and assess relevant published studies. It weighs the evidence from those studies to reach a considered statement on the subject of enquiry. The systematic reviewing protocol is first designed, registered and published (accessibly) so that the review can later be seen to conform to the original intent, removing temptations to ‘improve’ it later. All pertinent studies are then located, and a pair of researchers filter out those insufficiently relevant or informative; this filtering is itself described in the review manuscript. The selected studies then undergo evaluation of their scope, quality, biases, conclusions and soundness of their conclusions. 

### What is done with this information? {-}
The outcomes and information from the reviewed studies are synthesised to reach a nuanced conclusion for which the studies provide support. There is an assessment as to whether the evidence is strong enough to be acted upon in the clinic, whether further studies are desirable and whether further issues are raised, such as the consistency of the therapy outcomes.

### Is all that effort really needed? {-}
The reviewing process is time consuming but is needed because the biases and problems quality affecting the studies must be exposed to daylight and critically evaluated. This facilitates good integration of information to enable rational clinical decisions.

Developing a systematic review uses many tiresome and tedious techniques but demonstrably and drastically reduce bias. It is part of human psychology that biases are underestimated and ignored, and a key objective of evidence-based methodology to overcome this dangerous tendency, and such efforts are rarely glamorous. There is good evidence, for example, that studies that do not have fully blinded patient allocation to treatment and placebo arms display systematic bias and error. Note also that a systematic review can be regularly updated (i.e., repeated) as more studies are performed, leading eventually to consensus on best practices. 

### What types of studies are included in the systematic review? {-}
All relevant published data and grey literature, suitably filtered. ‘Grey’ literature refers to studies not formally published in conventional journals, such as published abstracts and conference presentations, non-peer-reviewed manuscripts (e.g., bioRrxiv) and studies not published in English. It is challenging to find and interpret such studies but it is a vital task. 

### What about meta-analyses? {-}
A meta-analysis goes a step further than a systematic review as it takes published data and aggregates and appraises it with a number of statistical tools to reach conclusions based on quantitative analysis. This is a powerful method that can result in strong conclusions if the studies under scrutiny are under- or poorly-powered small studies, so can provide stronger evidence than the individual studies alone as long as biases are recognised and dealt with. 

What happens if the studies that the systematic review reviews are no good?
Aggregation of bad data cannot magically produce good data, but part of the systematic process is to weigh the quality of the studies. Good data with low bias from  large studies contributes more to review conclusions than data from a poorly executed and wobbly study with few participants. However, aggregating of weak studies can provide useful data for meta-analysis, so collectively provide stronger evidence that the individual weak studies can.  

### How are the biases and other flaws in studies assessed? {-}
The standardised methods used reduce bias cannot easily be summarised here, but common approaches include standard checklists (e.g., to assess the quality of randomisation and blinding) or quantitative measures such as funnel plots that assess publication bias. Such particular methods are selected beforehand then used rigorously and with great care. 

In sum, a well performed systematic review is a crucial and high quality contribution to evidence based healthcare.

## Regression to the mean – a bewildering effect of random variation.
Randomness can play tricks on us and behave in some odd ways. A good example is the phenomenon of ‘regression towards the mean’, but also called ‘reversion to the mean’ which is a little easier to grasp as a term. This phenomenon is important to be aware of when designing clinical trials, and it is commonly encountered so it is useful to be able to spot it and avoid being fooled. To paraphrase its statistical description, an extreme event will nearly always be followed by a less extreme event.  While this bland description captures its essence, it does not portray very well how such a malicious trap lurks in the shadows.

### Terrible darts players {-}
Imagine that you are a very bad darts player, so can land a dart on the board, but exactly where it lands is pretty random. As a result the laws of random chance best describe the score you obtain from throwing three darts.  Imagine also that several friends, who are just as hopeless, join you to play a few rounds. On the sixth round one friend scores extremely well, but you do badly. The next round your friend, fired up but still hopeless, scores only a little better than average, as do you. On the seventeenth round you have a stunning win and a very high score. However, your triumph crumbles the next round, so you lose badly and slink miserably off to the bar.

The above scenario is familiar and easy to understand. It is plain why one or two high scores happened and why they are not repeated – they were just flukes. Over the long term if the score of each player is added up they will all tend toward same number, which is the average (or mean) of the group, however over the shorter period such flukes will be apparent. Freakishly low scores of course also happen, again followed by scores closer to the average.  There is no formal cause for your unusual high score – it was chance, and you had little or nothing to do with it. Note that here we assume that your skill levels in darts do not go up, though of course with time and persistence it will, and the scores less random.

### Punishment, reward and intolerant teachers {-}
To take the example a little further, imagine you have acquired a darts teacher who is completely ineffective but unaware of it. He yells at those students who score very badly and praises those who score really well. The laggards immediately improve, but the high achievers go backwards. A logical conclusion seems to be that praise doesn’t work and that yelling at bad players does, in defiance, it should be noted, of the insights of psychology. However, exactly the same pattern of improvement and worsening in your darts matches will be seen even when the teacher is not around to praise or punish. This is easy to explain if reversion to the mean is seen for what it is, but we seem almost hardwired to see things as the teacher does.

### Invisible randomness {-}
The problem always with randomness (i.e., non-systematic variation around the mean) is we often are blind to the magnitude of its effects and the role they play. We always look for explanations that involve simple causes, especially human agency. So the good scores we get are ascribed to positive thinking, our hard effort, or our rabbit foot, but we then forgot to keep thinking positively or took our eye off the ball, or the foot. The human brain is so good at spotting patterns it has no problem spotting patterns that do not exist, indeed many brains resist the very notion that random chance exists. Temporary successes in many fields of endeavour, even in research (who knows) can be susceptible to the warm embrace of good fortune followed by the cold shower of reversion to the mean.  Thus prizes can be awarded to the outstanding, who then go on to disappoint. This is not due to a jinx but our underestimation of how large a part randomness plays in performance. This happens with clinical data as well. 

### An aside: Francis Galton {-}
Regression to the mean was discovered and described by Francis Galton in the 19th century. As part of his otherwise unfortunate dedication to eugenics he made many measurements of height of parents and their children, noting that child heights usually lay between the mean of their parents and the population height mean. By mathematically describing these observations, he stumbled upon the ideas of simple regression that now play a fundamental part of statistical analysis.

### Influences on clinical trial design {-}
It is now widely understood that when setting up a randomised clinical trial the best design is (as the name suggests) to randomise the subjects at the outset, and to do so rigorously. This does make intuitive sense since it means the control and treatment group will be equivalent. 

If, however, we consider what might happen if this is not done in a disease treatment trial – if at the start the treatment group actually had worse symptoms than the control group, any apparent improvement may just be reversion to the mean and not a treatment effect. Random blind allocation minimises this, so is crucial. Effects of reversion to the mean can be compounded by putting study subjects into groups based on initial (baseline) parameters.  So, if patients are allocated to a study based on one measurement being above a threshold, some may be false positives at the start because they were by chance higher than usual for them. This can be avoided using allocation based on more than one parameter to reduce effects of reversion to the mean. 

### What to do {-}
Reversion to the mean is usually tackled by using careful protocol strategies to avoid it, some as outlined above. This is why the many time consuming tasks of double-blind patient allocation to treatment/placebo, pre-registration of trial protocols and other irksome duties need committed and careful adherence. Statistical analysis and estimation techniques exist to prevent problems around reversion to the mean; one useful and readable resource is noted below.

### Further entertainment{-}
There is much literature on this subject much of which is highly readable, in particular from giants in the field Daniel Kahneman and Amos Tversky, and Dan Ariely.

* Ref- Barnett A, Van Der Pols J, Dobson A, (2005) Regression to the mean: what it is and how to deal with it, International Journal of Epidemiology p215-220

## Making multiple comparisons and ANOVA testing 
One of the foundations upon which a medical study rests is the statistical testing of its data. This can unfortunately be undermined by a common problem seen even in highly respectable peer-reviewed literature. This problem arises from making multiple comparisons in the data. It is a reason why so many studies are hard or impossible to reproduce. Since a study that cannot be reproduced is a waste of time it is essential for researchers understand this issue, since it has a relatively easy to fix.

### Making single comparisons {-}
Statistical tests can assess whether or not a pattern in the data simply reflects a pattern of random variations. Working on random numbers is of no interest to anyone, so an objective methods are employed to spot random patterns since the human mind is very  bad at this and is easily fooled. 

A commonly used test is Student’s t-test for comparing data from two categorical variables. Thus, to see the effects of a drug on a patient parameter (such as their hematocrit), two groups are set up: patients taking the drug (the treatment group) and patients not taking it (the placebo group). Measurements of hematocrit are made from both groups of patients. The question is whether the drug affects the parameter of interest, here the hematocrit: formally, the question is whether the group receiving the treatment has an increased or decreased average (mean) hematocrit compared to that of the placebo group.  Since random data fluctuations from a variety of sources will also cause some variation in the group means, so we want to know a change in mean hematocrit is not explained by such random fluctuations. The t-test can do this job, but it does not return a yes/no answer but a p-value, which we have to interpret.

### P-values {-}
The t-test p-value is an estimate of how likely the difference between the means of treatment and placebo groups (or a greater difference) is due to random variation. A p-value less than 5% is commonly used to establish a true non-random drug effect, designated as ‘statistically significant’. Note that a p-value above 5% does not prove that the drug has no effect, it simply means that we have no evidence for any effect. With this caveat, a p-value of 5% represents a 1 in 20 chance that this conclusion is wrong, i.e., it was random after all. This is the source of the problem. 

### Multiple tests can spoil everything {-}
The t-test and the p-values it yields are widely understood, but often we want to test more than one parameter. Suppose 20 parameters are tested in patients taking that drug and that t-tests are performed on all and them and, furthermore, all those tests give a p-value of 5%. Having 20 comparisons each of which has a 1 in 20 chance of being wrong suggests that something will be wrong. In fact the chances they are all non-random true effects being observed is only about 1 in 3, and it is not at all clear which of those parameter are giving a false positive.

### Ronald Fisher’s potatoes {-}
Fisher was one of the founders of statistical science and invented many of its core concepts such as p-values. In the early 1920s he had data on the yield of 12 different varieties of potatoes and needed to find which was the most productive crop.  For this he used his ANOVA (or analysis of variance test), model. The mean and variance of the measurements of potato yield for each variety was calculated and the ANOVA test determined whether all the means were the same or if there was some difference. Thus, ANOVA compared how the individual mean differed from the overall mean of data all the varieties pooled together; the degree of variation between groups and within groups was also compared. The potato data analysed this way indicated there were differences between the fertilisers. However, the test did not reveal which gave varieties were best. 

This lack of information on individual potato types is a feature of an omnibus test that only gives useful general information about a dataset. This is like a biscuit jar in a share house found unusually empty, indicating that biscuit consumption has suddenly increased without identifying the culprit. However, it is best to determine that the biscuit jar really is low before leveling accusations. In the same way the ANOVA indicates that there is some statistical signal which can be followed up.

### Threshold correction {-}
To identify which potato is best, a post hoc test is need; the test is called this as it is done after the ANOVA screen. There are various approaches used, and some disagreement as to the most appropriate (it depends on the dataset) but a common approach is to adjust the p-value threshold to compensate for multiple testing and reduce the number of false positives. The False Discovery Rate is an algorithmic method used for large datasets. Also common is use of pairwise t-tests used to compare each pair of means (here, every two sets of potato variety) with a Bonferoni correction to the p-value threshold. In this case the threshold (here, 5%) is divided by the number of tests done and that adjusted p-value used to determine significance. Other tests exist (termed post hoc tests) , such as Tukey’s, which is more conservative, i.e., harder to obtain a significant result.

### 1-way and 2-way ANOVA {-}
Fisher’s fun with potatoes did not end with potato varieties since there are different types of fertilisers used with the potatoes, and the varieties varied in their response to the fertilisers. This provides an extra dimension (or category) to the data results in  a two-way comparison, since the potato yield can vary due both to potato type and fertiliser. This cannot be done with the simple ANOVA as described above and requires a 2-way ANOVA test. Similarly in clinical data there may be both treatment and patient sex to take into account because there may be a difference in treatment response between the two sexes. 2-way is more work to calculate than 1-way ANOVA (horrifyingly so in Fisher’s day) but when using computer software the work is only slightly more.

### Not using ANOVA {-}
The main assumption underlying ANOVA is that like t-test there is an assumption that the data is normally distributed. If this is not true, a test such as Kruskal-Wallis with paired Mann-Whitney tests can be used. These tests data ranks rather than the data itself. Which test to use may depend on the software available but it would be wise to get some advice on this.  There are other approaches to examining multiple comparisons, such as methods based on Bayesian statistics, but this is a large subject in itself.